{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09c3663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a8509\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator, get_single_color_func\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import re\n",
    "import math\n",
    "import streamlit as st\n",
    "from imageio import imread\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6b3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "import streamlit_wordcloud as wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dd3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataframe\n",
    "dataset = 'scrapeDF_273_Iphone'\n",
    "dataset_name = dataset.split('_')\n",
    "dataset_name = dataset_name[len(dataset_name) - 1]\n",
    "path = 'data/'\n",
    "df = pd.read_csv(path + dataset + '.csv', index_col=0)\n",
    "newData = df.groupby('stream')\n",
    "reddit_df = list(df.groupby('stream'))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b34144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set n-gram\n",
    "ngram_name = 'Unigram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aabcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set n-gram\n",
    "if ngram_name.lower() == 'unigram':\n",
    "    ngram = 1\n",
    "elif ngram_name.lower() == 'bigram':\n",
    "    ngram = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e519d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = dataset_name\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "STOP_WORDS.extend(list(STOPWORDS))\n",
    "STOP_WORDS = set(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bd1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    brand_lower = brand.lower()\n",
    "    temp = text.lower()\n",
    "    if brand.lower() == 'iphone':\n",
    "        temp = re.sub(brand_lower + ' [0-9]+', '', temp) # for iphone model\n",
    "        temp = re.sub(brand_lower + '[0-9]+', '', temp) # for iphone model\n",
    "        temp = re.sub('apple', '', temp)\n",
    "    temp = re.sub(brand_lower, ' ', temp)\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp) # mentions\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp) # hashtags\n",
    "    temp = re.sub(r\"http\\S+\", \"\", temp) # weblinks\n",
    "    temp = re.sub(r\"www.\\S+\", \"\", temp) # websites\n",
    "    temp = re.sub('[()!?]', ' ', temp) # punctuation\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub('\\.\\Z', '', temp) # remove dot but not remove decimal point\n",
    "    temp = re.sub('\\.\\s+', ' ', temp) # remove dot but not remove decimal point\n",
    "    reg_float='^\\d+\\.\\d+$'\n",
    "    temp = re.sub(\"[^a-z0-9.]\",\" \", temp) # non alpha numeric dot\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in STOP_WORDS]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc3c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n-gram words\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    ngrams = zip(*[text[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76485e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['text_with_stopw'] = reddit_df.text.apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0bb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveDuplicate(list_of_words):\n",
    "    final_list = []\n",
    "    for word in list_of_words:\n",
    "        if word not in final_list:\n",
    "            final_list.append(word)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba42648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "original_reddit_words = []\n",
    "reddit_ngram = defaultdict(int)\n",
    "\n",
    "for tweet in reddit_df['text_with_stopw']:\n",
    "    for word in generate_ngrams(tweet.split(), n_gram=ngram):\n",
    "        original_reddit_words.append(word)\n",
    "        reddit_ngram[word] += 1\n",
    "\n",
    "reddit_words = RemoveDuplicate(original_reddit_words)\n",
    "reddit_cleaned_lines = dict()\n",
    "reddit_pos_words = []\n",
    "reddit_neg_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01d4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for r in reddit_words:\n",
    "    if not r.lower() in STOP_WORDS:\n",
    "        neg_polarity = sia.polarity_scores(r)['neg']\n",
    "        pos_polarity = sia.polarity_scores(r)['pos']\n",
    "        cpd_polarity = sia.polarity_scores(r)['compound']\n",
    "        sentiment = (neg_polarity+pos_polarity)*cpd_polarity\n",
    "        \n",
    "        if(sentiment != 0):\n",
    "            reddit_cleaned_lines[r] = sentiment            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02df2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit        \n",
    "for key, value in reddit_cleaned_lines.items():\n",
    "    if(value > 0):\n",
    "        reddit_pos_words.append(key)\n",
    "    else:\n",
    "        reddit_neg_words.append(key)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a439d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 21:27:10.343 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "st.set_page_config(layout=\"centered\")\n",
    "\n",
    "st.title(f'{ngram_name} reddit WordCloud')\n",
    "# Using Wordcloud component\n",
    "words = []\n",
    "\n",
    "for word, val in reddit_ngram.items():\n",
    "    word_color = 'green' # neutral words' color\n",
    "    sentiment = 'neutral'\n",
    "    if word in reddit_pos_words:\n",
    "        word_color = 'orange' # positive words' color\n",
    "        sentiment = 'positive'\n",
    "    elif word in reddit_neg_words:\n",
    "        word_color = 'black' # negative words' color\n",
    "        sentiment = 'negative'\n",
    "        \n",
    "    dic = dict(text=word, value=val, color=word_color, sentiment=sentiment)\n",
    "    words.append(dic)\n",
    "max_words = 0\n",
    "if ngram_name.lower() == 'unigram':\n",
    "    max_words = 500\n",
    "elif ngram_name.lower() == 'bigram':\n",
    "    max_words = 100     \n",
    "\n",
    "return_obj = wordcloud.visualize(words,width='100%', tooltip_data_fields={\n",
    "    'text':'Word', 'value':'Count', 'sentiment':'Sentiment'}, per_word_coloring=True, max_words = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8f54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
